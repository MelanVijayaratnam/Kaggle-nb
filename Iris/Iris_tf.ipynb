{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Logistic Regression with TensorFlow</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Introduction</u></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to familiarize with the <u>TensorFlow</u>, the Deep Learning library build by Google in November 2015. <br>\n",
    "Now it is the framework that it is the most used and we will use it in order to build a Logistic Regression model using the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Package</u></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import the packages that we need to achieve our goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Iris dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('Iris.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify things, we will only consider 2 attributes and 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpJBSu4iIhmk5C4ikkH/HwzZ3k53KeYVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa226ce5860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data.iloc[:100, 1:3].values\n",
    "y = data.iloc[:100, 5].values\n",
    "\n",
    "m = 100\n",
    "\n",
    "setosa = plt.scatter(X[:50, 0], X[:50, 1], c='b', label=\"setosa\")\n",
    "versicolor = plt.scatter(X[50:, 0], X[50:, 1], c='r', label=\"versicolor\")\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to add an extra bias feature ($x_0 = 1$) to every instance. <br>\n",
    "For this, we just need to add a column full of 1s on the left of the input matrix <b>X</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones((m, 1)), X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reshape y_train to make it a column vector (i.e. a 2D array with a single column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_column_vector = y.reshape(-1, 1)\n",
    "y_column_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert the Species into number, in this case, we will impose <br>\n",
    "'Iris-setosa' = 1 and 'Iris-versicolor' = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_bin = (np.where(y_column_vector == 'Iris-setosa', 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data = np.concatenate((X_with_bias, y_bin), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  5.1,  3.5,  1. ],\n",
       "       [ 1. ,  4.9,  3. ,  1. ],\n",
       "       [ 1. ,  4.7,  3.2,  1. ],\n",
       "       [ 1. ,  4.6,  3.1,  1. ],\n",
       "       [ 1. ,  5. ,  3.6,  1. ],\n",
       "       [ 1. ,  5.4,  3.9,  1. ],\n",
       "       [ 1. ,  4.6,  3.4,  1. ],\n",
       "       [ 1. ,  5. ,  3.4,  1. ],\n",
       "       [ 1. ,  4.4,  2.9,  1. ],\n",
       "       [ 1. ,  4.9,  3.1,  1. ],\n",
       "       [ 1. ,  5.4,  3.7,  1. ],\n",
       "       [ 1. ,  4.8,  3.4,  1. ],\n",
       "       [ 1. ,  4.8,  3. ,  1. ],\n",
       "       [ 1. ,  4.3,  3. ,  1. ],\n",
       "       [ 1. ,  5.8,  4. ,  1. ],\n",
       "       [ 1. ,  5.7,  4.4,  1. ],\n",
       "       [ 1. ,  5.4,  3.9,  1. ],\n",
       "       [ 1. ,  5.1,  3.5,  1. ],\n",
       "       [ 1. ,  5.7,  3.8,  1. ],\n",
       "       [ 1. ,  5.1,  3.8,  1. ],\n",
       "       [ 1. ,  5.4,  3.4,  1. ],\n",
       "       [ 1. ,  5.1,  3.7,  1. ],\n",
       "       [ 1. ,  4.6,  3.6,  1. ],\n",
       "       [ 1. ,  5.1,  3.3,  1. ],\n",
       "       [ 1. ,  4.8,  3.4,  1. ],\n",
       "       [ 1. ,  5. ,  3. ,  1. ],\n",
       "       [ 1. ,  5. ,  3.4,  1. ],\n",
       "       [ 1. ,  5.2,  3.5,  1. ],\n",
       "       [ 1. ,  5.2,  3.4,  1. ],\n",
       "       [ 1. ,  4.7,  3.2,  1. ],\n",
       "       [ 1. ,  4.8,  3.1,  1. ],\n",
       "       [ 1. ,  5.4,  3.4,  1. ],\n",
       "       [ 1. ,  5.2,  4.1,  1. ],\n",
       "       [ 1. ,  5.5,  4.2,  1. ],\n",
       "       [ 1. ,  4.9,  3.1,  1. ],\n",
       "       [ 1. ,  5. ,  3.2,  1. ],\n",
       "       [ 1. ,  5.5,  3.5,  1. ],\n",
       "       [ 1. ,  4.9,  3.1,  1. ],\n",
       "       [ 1. ,  4.4,  3. ,  1. ],\n",
       "       [ 1. ,  5.1,  3.4,  1. ],\n",
       "       [ 1. ,  5. ,  3.5,  1. ],\n",
       "       [ 1. ,  4.5,  2.3,  1. ],\n",
       "       [ 1. ,  4.4,  3.2,  1. ],\n",
       "       [ 1. ,  5. ,  3.5,  1. ],\n",
       "       [ 1. ,  5.1,  3.8,  1. ],\n",
       "       [ 1. ,  4.8,  3. ,  1. ],\n",
       "       [ 1. ,  5.1,  3.8,  1. ],\n",
       "       [ 1. ,  4.6,  3.2,  1. ],\n",
       "       [ 1. ,  5.3,  3.7,  1. ],\n",
       "       [ 1. ,  5. ,  3.3,  1. ],\n",
       "       [ 1. ,  7. ,  3.2,  0. ],\n",
       "       [ 1. ,  6.4,  3.2,  0. ],\n",
       "       [ 1. ,  6.9,  3.1,  0. ],\n",
       "       [ 1. ,  5.5,  2.3,  0. ],\n",
       "       [ 1. ,  6.5,  2.8,  0. ],\n",
       "       [ 1. ,  5.7,  2.8,  0. ],\n",
       "       [ 1. ,  6.3,  3.3,  0. ],\n",
       "       [ 1. ,  4.9,  2.4,  0. ],\n",
       "       [ 1. ,  6.6,  2.9,  0. ],\n",
       "       [ 1. ,  5.2,  2.7,  0. ],\n",
       "       [ 1. ,  5. ,  2. ,  0. ],\n",
       "       [ 1. ,  5.9,  3. ,  0. ],\n",
       "       [ 1. ,  6. ,  2.2,  0. ],\n",
       "       [ 1. ,  6.1,  2.9,  0. ],\n",
       "       [ 1. ,  5.6,  2.9,  0. ],\n",
       "       [ 1. ,  6.7,  3.1,  0. ],\n",
       "       [ 1. ,  5.6,  3. ,  0. ],\n",
       "       [ 1. ,  5.8,  2.7,  0. ],\n",
       "       [ 1. ,  6.2,  2.2,  0. ],\n",
       "       [ 1. ,  5.6,  2.5,  0. ],\n",
       "       [ 1. ,  5.9,  3.2,  0. ],\n",
       "       [ 1. ,  6.1,  2.8,  0. ],\n",
       "       [ 1. ,  6.3,  2.5,  0. ],\n",
       "       [ 1. ,  6.1,  2.8,  0. ],\n",
       "       [ 1. ,  6.4,  2.9,  0. ],\n",
       "       [ 1. ,  6.6,  3. ,  0. ],\n",
       "       [ 1. ,  6.8,  2.8,  0. ],\n",
       "       [ 1. ,  6.7,  3. ,  0. ],\n",
       "       [ 1. ,  6. ,  2.9,  0. ],\n",
       "       [ 1. ,  5.7,  2.6,  0. ],\n",
       "       [ 1. ,  5.5,  2.4,  0. ],\n",
       "       [ 1. ,  5.5,  2.4,  0. ],\n",
       "       [ 1. ,  5.8,  2.7,  0. ],\n",
       "       [ 1. ,  6. ,  2.7,  0. ],\n",
       "       [ 1. ,  5.4,  3. ,  0. ],\n",
       "       [ 1. ,  6. ,  3.4,  0. ],\n",
       "       [ 1. ,  6.7,  3.1,  0. ],\n",
       "       [ 1. ,  6.3,  2.3,  0. ],\n",
       "       [ 1. ,  5.6,  3. ,  0. ],\n",
       "       [ 1. ,  5.5,  2.5,  0. ],\n",
       "       [ 1. ,  5.5,  2.6,  0. ],\n",
       "       [ 1. ,  6.1,  3. ,  0. ],\n",
       "       [ 1. ,  5.8,  2.6,  0. ],\n",
       "       [ 1. ,  5. ,  2.3,  0. ],\n",
       "       [ 1. ,  5.6,  2.7,  0. ],\n",
       "       [ 1. ,  5.7,  3. ,  0. ],\n",
       "       [ 1. ,  5.7,  2.9,  0. ],\n",
       "       [ 1. ,  6.2,  2.9,  0. ],\n",
       "       [ 1. ,  5.1,  2.5,  0. ],\n",
       "       [ 1. ,  5.7,  2.8,  0. ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Train/Test set split</u></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the loaded dataset into two, <b>80%</b> of which we will use to train our model and <b>20%</b> that we will hold back as a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42) # To make output consistent in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    \n",
    "    return data[train_indices], data[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (80, 4) Test size: (20, 4)\n"
     ]
    }
   ],
   "source": [
    "test_ratio = 0.2\n",
    "train, test = split_train_test(full_data, test_ratio=test_ratio)\n",
    "print(\"Train size:\", train.shape, \"Test size:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (80, 4) y_train.shape: (80, 1)\n",
      "X_test.shape: (20, 4) y_test.shape: (20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train[:, 0:5], train[:, -1].reshape(-1, 1)\n",
    "X_test, y_test = test[:, 0:5], test[:, -1].reshape(-1, 1)\n",
    "print(\"X_train.shape:\", X_train.shape, \"y_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape, \"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>TensorFlow</u></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's the reset the default graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our preprocessed data has two input features, the sepal length and the sepal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Logistic Regression computes a weighted sum of the inputs and then it applies the sigmoid function to the result, which gives us the estimated probability for the positive class:\n",
    "$\\hat p = h_{\\boldsymbol{\\theta}}(x) = \\sigma(\\boldsymbol{\\theta}^{T}.\\boldsymbol{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $\\boldsymbol{\\theta}$ is the parameter vector, containing the bias term $\\theta_0$ and the weights $\\theta_1, \\theta_2, ..., \\theta_n$. <br>\n",
    "The input vector $\\boldsymbol{x}$ contains a constant term $x_0 = 1$, as well as all the input features $x_1, x_2, ..., x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the <b>logistic_regression()</b> function to create the graph. We will leave out the definition of the inputs X and the targets y. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    \n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        \n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "                \n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('loss_loss', loss)\n",
    "            \n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "            \n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a little function to get the name of the log directory to save the summaries for Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the graph using the <b>logistic_regression()</b> function. We will also create the <b>FileWriter</b> to save the summaries to the log directory for Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2 \n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
